{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CART (Classification and Regression Trees) - DecisionTree Classifier\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 2.995\n",
      "Confusion Matrix:\n",
      "[[48  1  7]\n",
      " [ 3  0  2]\n",
      " [ 8  0 14]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83        56\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.61      0.64      0.62        22\n",
      "\n",
      "    accuracy                           0.75        83\n",
      "   macro avg       0.47      0.50      0.49        83\n",
      "weighted avg       0.71      0.75      0.73        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Initialize the Decision Tree Classifier with hyperparameters\n",
    "max_depth = 5  # You can adjust this value\n",
    "min_samples_split = 2  # You can adjust this value\n",
    "min_samples_leaf = 1  # You can adjust this value\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "random_seed = 50\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_dt = confusion_matrix(Y_test, model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_dt)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_dt = classification_report(Y_test, model.predict(X_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gaussian Naive Bayes\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 0.703\n",
      "Confusion Matrix:\n",
      "[[40  0  4]\n",
      " [ 6  0  1]\n",
      " [14  0 18]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        44\n",
      "           1       1.00      0.00      0.00         7\n",
      "           2       0.78      0.56      0.65        32\n",
      "\n",
      "    accuracy                           0.70        83\n",
      "   macro avg       0.82      0.49      0.47        83\n",
      "weighted avg       0.74      0.70      0.66        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "# Standardize the features before using Naive Bayes\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Standardize the training and testing data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create a Gaussian Naive Bayes classifier\n",
    "    model = GaussianNB(priors=None, var_smoothing=1e-9)\n",
    "\n",
    "    # Calibrate the classifier\n",
    "    calibrated_model = CalibratedClassifierCV(estimator=model, method='sigmoid')\n",
    "    calibrated_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = calibrated_model.predict_proba(X_test_scaled)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_nb = confusion_matrix(Y_test, calibrated_model.predict(X_test_scaled))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_nb)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_nb = classification_report(Y_test, calibrated_model.predict(X_test_scaled), zero_division=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Boosting Machines (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 0.858\n",
      "Confusion Matrix:\n",
      "[[35  1  8]\n",
      " [ 2  4  1]\n",
      " [ 7  0 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        44\n",
      "           1       0.80      0.57      0.67         7\n",
      "           2       0.74      0.78      0.76        32\n",
      "\n",
      "    accuracy                           0.77        83\n",
      "   macro avg       0.78      0.72      0.74        83\n",
      "weighted avg       0.77      0.77      0.77        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create an AdaBoost classifier\n",
    "    model = AdaBoostClassifier(n_estimators=50, random_state=seed)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_ab = confusion_matrix(Y_test, model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_ab)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_ab = classification_report(Y_test, model.predict(X_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Nearest Neighbors (K-NN)\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 3.497\n",
      "Confusion Matrix:\n",
      "[[36  0  8]\n",
      " [ 7  0  0]\n",
      " [15  0 17]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.71        44\n",
      "           1       1.00      0.00      0.00         7\n",
      "           2       0.68      0.53      0.60        32\n",
      "\n",
      "    accuracy                           0.64        83\n",
      "   macro avg       0.77      0.45      0.43        83\n",
      "weighted avg       0.68      0.64      0.60        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create a K-Nearest Neighbors (K-NN) classifier\n",
    "    model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_knn = confusion_matrix(Y_test, model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_knn)\n",
    "\n",
    "# Generate the classification report for the last split with zero_division parameter\n",
    "classification_report_knn = classification_report(Y_test, model.predict(X_test), zero_division=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 0.563\n",
      "Confusion Matrix:\n",
      "[[41  0  3]\n",
      " [ 6  1  0]\n",
      " [10  1 21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81        44\n",
      "           1       0.50      0.14      0.22         7\n",
      "           2       0.88      0.66      0.75        32\n",
      "\n",
      "    accuracy                           0.76        83\n",
      "   macro avg       0.70      0.58      0.59        83\n",
      "weighted avg       0.76      0.76      0.74        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create a Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs', C=1.0)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_lr = confusion_matrix(Y_test, model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_lr)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_lr = classification_report(Y_test, model.predict(X_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Multi-Layer Perceptron (MLP)\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 7.086\n",
      "Confusion Matrix:\n",
      "[[10  5 29]\n",
      " [ 1  1  5]\n",
      " [ 3  1 28]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.23      0.34        44\n",
      "           1       0.14      0.14      0.14         7\n",
      "           2       0.45      0.88      0.60        32\n",
      "\n",
      "    accuracy                           0.47        83\n",
      "   macro avg       0.44      0.42      0.36        83\n",
      "weighted avg       0.56      0.47      0.42        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create an MLP-based model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(65, 32), activation='relu', solver='adam', max_iter=200, random_state=seed)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_mlp = confusion_matrix(Y_test, model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_mlp)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_mlp = classification_report(Y_test, model.predict(X_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Perceptron\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Label: C, Numerical Value: 0\n",
      "Label: CL, Numerical Value: 1\n",
      "Label: D, Numerical Value: 2\n",
      "Mean Logarithmic Loss: 0.700\n",
      "Confusion Matrix:\n",
      "[[40  0  4]\n",
      " [ 3  0  4]\n",
      " [12  0 20]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.91      0.81        44\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.71      0.62      0.67        32\n",
      "\n",
      "    accuracy                           0.72        83\n",
      "   macro avg       0.48      0.51      0.49        83\n",
      "weighted avg       0.66      0.72      0.69        83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Display the mapping of labels to numerical values\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "for label, value in label_mapping.items():\n",
    "    print(f\"Label: {label}, Numerical Value: {value}\")\n",
    "\n",
    "# Repeated Random Train-Test splits using RepeatedKFold\n",
    "n_splits = 5  # Number of splits\n",
    "n_repeats = 10  # Number of repeats\n",
    "seed = 7\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "log_losses = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create a Perceptron classifier\n",
    "    model = Perceptron(max_iter=200, random_state=seed, eta0=1.0, tol=1e-3)\n",
    "\n",
    "    # Fit the Perceptron model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Calibrate the classifier to get probability estimates\n",
    "    calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "\n",
    "    # Check if the model is fitted\n",
    "    check_is_fitted(model, attributes=[\"coef_\", \"intercept_\"])\n",
    "\n",
    "    calibrated_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the log loss\n",
    "    y_pred_proba = calibrated_model.predict_proba(X_test)\n",
    "    log_loss_value = log_loss(Y_test, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "\n",
    "# Calculate and print the mean log loss\n",
    "mean_log_loss = sum(log_losses) / len(log_losses)\n",
    "print(\"Mean Logarithmic Loss: %.3f\" % mean_log_loss)\n",
    "\n",
    "# Generate the confusion matrix for the last split\n",
    "confusion_matrix_perceptron = confusion_matrix(Y_test, calibrated_model.predict(X_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_perceptron)\n",
    "\n",
    "# Generate the classification report for the last split\n",
    "classification_report_perceptron = classification_report(Y_test, calibrated_model.predict(X_test), zero_division='warn')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_perceptron)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Random Forest\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logarithmic Loss: 0.63732\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Set the test size\n",
    "test_size = 0.20  # Hyperparameter: Fraction of the dataset to use for testing\n",
    "n_splits = 5      # Number of splits for repeated random train-test splits\n",
    "n_repeats = 3     # Number of times to repeat the cross-validation process\n",
    "\n",
    "# Initialize RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rfmodel = RandomForestClassifier(n_estimators=100, random_state=seed, max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# Initialize lists to store log loss values\n",
    "log_loss_values = []\n",
    "\n",
    "# Perform repeated random train-test splits\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    rfmodel.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict probabilities for log loss calculation\n",
    "    Y_probabilities = rfmodel.predict_proba(X_test)\n",
    "\n",
    "    # Calculate log loss\n",
    "    loss = log_loss(Y_test, Y_probabilities)\n",
    "    log_loss_values.append(loss)\n",
    "\n",
    "# Calculate mean log loss\n",
    "mean_log_loss = np.mean(log_loss_values)\n",
    "print(\"Mean Logarithmic Loss: %.5f\" % mean_log_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Support Vector Machines (SVM)\n",
    "- Sampling Technique - Train/Test Split (80:20)\n",
    "- Classification Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logarithmic Loss: 0.56745\n",
      "\n",
      "Classification Report for Split 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85        47\n",
      "           1       0.00      0.00      1.00         2\n",
      "           2       0.92      0.69      0.79        35\n",
      "\n",
      "    accuracy                           0.81        84\n",
      "   macro avg       0.57      0.54      0.88        84\n",
      "weighted avg       0.82      0.81      0.83        84\n",
      "\n",
      "\n",
      "Classification Report for Split 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81        42\n",
      "           1       1.00      0.00      0.00        12\n",
      "           2       0.86      0.63      0.73        30\n",
      "\n",
      "    accuracy                           0.73        84\n",
      "   macro avg       0.85      0.54      0.51        84\n",
      "weighted avg       0.79      0.73      0.66        84\n",
      "\n",
      "\n",
      "Classification Report for Split 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        48\n",
      "           1       1.00      0.00      0.00         3\n",
      "           2       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.89      0.55      0.55        84\n",
      "weighted avg       0.84      0.82      0.80        84\n",
      "\n",
      "\n",
      "Classification Report for Split 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.93      0.82        42\n",
      "           1       1.00      0.00      0.00         5\n",
      "           2       0.87      0.72      0.79        36\n",
      "\n",
      "    accuracy                           0.78        83\n",
      "   macro avg       0.87      0.55      0.54        83\n",
      "weighted avg       0.81      0.78      0.76        83\n",
      "\n",
      "\n",
      "Classification Report for Split 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85        53\n",
      "           1       1.00      0.00      0.00         3\n",
      "           2       0.69      0.67      0.68        27\n",
      "\n",
      "    accuracy                           0.78        83\n",
      "   macro avg       0.84      0.52      0.51        83\n",
      "weighted avg       0.79      0.78      0.77        83\n",
      "\n",
      "\n",
      "Classification Report for Split 6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85        47\n",
      "           1       1.00      0.00      0.00         2\n",
      "           2       0.85      0.66      0.74        35\n",
      "\n",
      "    accuracy                           0.80        84\n",
      "   macro avg       0.87      0.53      0.53        84\n",
      "weighted avg       0.81      0.80      0.78        84\n",
      "\n",
      "\n",
      "Classification Report for Split 7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.96      0.84        48\n",
      "           1       1.00      0.00      0.00         6\n",
      "           2       0.87      0.67      0.75        30\n",
      "\n",
      "    accuracy                           0.79        84\n",
      "   macro avg       0.87      0.54      0.53        84\n",
      "weighted avg       0.81      0.79      0.75        84\n",
      "\n",
      "\n",
      "Classification Report for Split 8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87        50\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.77      0.80      0.79        30\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.87      0.57      0.55        84\n",
      "weighted avg       0.83      0.82      0.80        84\n",
      "\n",
      "\n",
      "Classification Report for Split 9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82        41\n",
      "           1       1.00      0.00      0.00         7\n",
      "           2       0.86      0.71      0.78        35\n",
      "\n",
      "    accuracy                           0.77        83\n",
      "   macro avg       0.86      0.56      0.53        83\n",
      "weighted avg       0.80      0.77      0.74        83\n",
      "\n",
      "\n",
      "Classification Report for Split 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.91      0.81        46\n",
      "           1       1.00      0.00      0.00         6\n",
      "           2       0.76      0.61      0.68        31\n",
      "\n",
      "    accuracy                           0.73        83\n",
      "   macro avg       0.83      0.51      0.50        83\n",
      "weighted avg       0.76      0.73      0.70        83\n",
      "\n",
      "\n",
      "Classification Report for Split 11:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        51\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.95      0.62      0.75        29\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.91      0.54      0.54        84\n",
      "weighted avg       0.85      0.82      0.79        84\n",
      "\n",
      "\n",
      "Classification Report for Split 12:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85        50\n",
      "           1       1.00      0.00      0.00         8\n",
      "           2       0.75      0.69      0.72        26\n",
      "\n",
      "    accuracy                           0.77        84\n",
      "   macro avg       0.84      0.54      0.52        84\n",
      "weighted avg       0.79      0.77      0.73        84\n",
      "\n",
      "\n",
      "Classification Report for Split 13:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84        50\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.79      0.63      0.70        30\n",
      "\n",
      "    accuracy                           0.77        84\n",
      "   macro avg       0.85      0.52      0.51        84\n",
      "weighted avg       0.79      0.77      0.75        84\n",
      "\n",
      "\n",
      "Classification Report for Split 14:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.88      0.79        42\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.77      0.65      0.71        37\n",
      "\n",
      "    accuracy                           0.73        83\n",
      "   macro avg       0.83      0.51      0.50        83\n",
      "weighted avg       0.75      0.73      0.71        83\n",
      "\n",
      "\n",
      "Classification Report for Split 15:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.87      0.76        39\n",
      "           1       0.00      0.00      1.00         5\n",
      "           2       0.88      0.72      0.79        39\n",
      "\n",
      "    accuracy                           0.75        83\n",
      "   macro avg       0.52      0.53      0.85        83\n",
      "weighted avg       0.73      0.75      0.79        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Load the dataset\n",
    "filename = './cirrhosis.csv'\n",
    "dataframe = pd.read_csv(filename)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataframe.drop('Status', axis=1)\n",
    "Y = dataframe['Status']\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Encode the target variable (Status) using label encoding\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "# Set the test size\n",
    "test_size = 0.20  # Hyperparameter: Fraction of the dataset to use for testing\n",
    "n_splits = 5      # Number of splits for repeated random train-test splits\n",
    "n_repeats = 3     # Number of times to repeat the cross-validation process\n",
    "seed = 7\n",
    "\n",
    "# Initialize RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "# Create an SVM classifier\n",
    "model = SVC(kernel='linear', C=0.1, probability=True, random_state=seed, max_iter=10000)  # Increase max_iter further\n",
    "\n",
    "# Initialize lists to store log loss values and classification reports\n",
    "log_loss_values = []\n",
    "classification_reports = []\n",
    "\n",
    "# Suppress ConvergenceWarnings for now\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Perform repeated random train-test splits\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict probabilities for log loss calculation\n",
    "    Y_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    # Calculate log loss\n",
    "    loss = log_loss(Y_test, Y_probabilities)\n",
    "    log_loss_values.append(loss)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(Y_test, model.predict(X_test), zero_division=1)  # Adjust zero_division as needed\n",
    "    classification_reports.append(report)\n",
    "\n",
    "# Reset warning filters\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# Calculate mean log loss\n",
    "mean_log_loss = np.mean(log_loss_values)\n",
    "print(\"Mean Logarithmic Loss: %.5f\" % mean_log_loss)\n",
    "\n",
    "# Print the classification reports for each split\n",
    "for idx, report in enumerate(classification_reports):\n",
    "    print(f\"\\nClassification Report for Split {idx + 1}:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
